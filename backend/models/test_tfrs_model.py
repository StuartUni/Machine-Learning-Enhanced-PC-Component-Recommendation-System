# Created by: Stuart Smith
# Student ID: S2336002
# Date Created: 30/03/2025
# Description:
# This script tests a trained TFRS model using Precision@10 and NDCG@10.
# It loads the saved model, evaluates predictions against known ratings,
# and prints key performance metrics for deep learning-based recommendations.
# Debug logging has been added to understand why precision and ndcg may be low.

import sys
import os
import sqlite3
import pandas as pd
import tensorflow as tf
import tensorflow_recommenders as tfrs


CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_DIR = os.path.abspath(os.path.join(CURRENT_DIR, ".."))
if BACKEND_DIR not in sys.path:
    sys.path.insert(0, BACKEND_DIR)
    
from models.train_tfrs_model import BuildRankingModel

# ‚úÖ Paths
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DB_PATH = os.path.join(BASE_DIR, "database", "users.db")
MODEL_PATH = os.path.join(BASE_DIR, "models", "tfrs_model.keras")

# ‚úÖ Load ratings
def load_ratings():
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql("SELECT user_id, build_id, rating FROM ratings", conn)
    conn.close()
    return df

df = load_ratings()
df["user_id"] = df["user_id"].astype(str)
df["build_id"] = df["build_id"].astype(str)

# ‚úÖ Load trained model
model = tf.keras.models.load_model(
    MODEL_PATH, custom_objects={"BuildRankingModel": BuildRankingModel}
)

# ‚úÖ Create test dataset
test_data = tf.data.Dataset.from_tensor_slices({
    "user_id": df["user_id"],
    "build_id": df["build_id"],
    "rating": df["rating"]
}).batch(1)

# ‚úÖ Evaluation
def evaluate_model(model, test_data, k=5):
    hits, ndcgs = [], []

    print("\nüîç Starting evaluation...\n")
    for batch in test_data:
        user_id = batch["user_id"].numpy()[0].decode()
        true_build_id = batch["build_id"].numpy()[0].decode()

        _, build_ids = model.recommend(tf.constant([user_id]), k=k)
        build_ids = [b.decode() for b in build_ids[0].numpy()]  # ‚úÖ Decode to str

        print(f"üë§ User: {user_id}")
        print(f"‚úÖ True Build ID: {true_build_id}")
        print(f"üîÆ Predicted Top-{k} Build IDs: {build_ids}\n")

        hit = true_build_id in build_ids
        hits.append(hit)

        if hit:
            rank = list(build_ids).index(true_build_id) + 1
            ndcg = tf.math.log(2.0) / tf.math.log(tf.cast(rank + 1, tf.float32))
        else:
            ndcg = 0.0
        ndcgs.append(ndcg)

    precision_at_k = sum(hits) / len(hits)
    ndcg_at_k = sum(ndcgs) / len(ndcgs)
    return precision_at_k, ndcg_at_k

# ‚úÖ Run and print results
precision, ndcg = evaluate_model(model, test_data, k=10)
print(f"\n‚úÖ Precision@10: {precision:.4f}")
print(f"‚úÖ NDCG@10: {ndcg:.4f}")

