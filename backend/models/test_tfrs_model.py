# Created by: Stuart Smith
# Student ID: S2336002
# Date Created: 30/03/2025
# Description:
# This script tests a trained TFRS model using Precision@5 and NDCG@5.
# It loads the saved model, evaluates predictions against known ratings,
# and prints key performance metrics for deep learning-based recommendations.
# Debug logging has been added to understand why precision and ndcg may be low.

import os
import sqlite3
import pandas as pd
import tensorflow as tf
import tensorflow_recommenders as tfrs
from backend.models.train_tfrs_model import BuildRankingModel

# âœ… Paths
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DB_PATH = os.path.join(BASE_DIR, "database", "users.db")
MODEL_PATH = os.path.join(BASE_DIR, "models", "tfrs_model.keras")

# âœ… Load ratings
def load_ratings():
    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql("SELECT user_id, build_id, rating FROM ratings", conn)
    conn.close()
    return df

df = load_ratings()
df["user_id"] = df["user_id"].astype(str)
df["build_id"] = df["build_id"].astype(str)

# âœ… Load trained model
model = tf.keras.models.load_model(
    MODEL_PATH, custom_objects={"BuildRankingModel": BuildRankingModel}
)

# âœ… Create test dataset
test_data = tf.data.Dataset.from_tensor_slices({
    "user_id": df["user_id"],
    "build_id": df["build_id"],
    "rating": df["rating"]
}).batch(1)

# âœ… Evaluation
def evaluate_model(model, test_data, k=5):
    hits, ndcgs = [], []

    print("\nğŸ” Starting evaluation...\n")
    for batch in test_data:
        user_id = batch["user_id"].numpy()[0].decode()
        true_build_id = batch["build_id"].numpy()[0].decode()

        _, build_ids = model.recommend(tf.constant([user_id]), k=k)
        build_ids = [b.decode() for b in build_ids[0].numpy()]  # âœ… Decode to str

        print(f"ğŸ‘¤ User: {user_id}")
        print(f"âœ… True Build ID: {true_build_id}")
        print(f"ğŸ”® Predicted Top-{k} Build IDs: {build_ids}\n")

        hit = true_build_id in build_ids
        hits.append(hit)

        if hit:
            rank = list(build_ids).index(true_build_id) + 1
            ndcg = tf.math.log(2.0) / tf.math.log(tf.cast(rank + 1, tf.float32))
        else:
            ndcg = 0.0
        ndcgs.append(ndcg)

    precision_at_k = sum(hits) / len(hits)
    ndcg_at_k = sum(ndcgs) / len(ndcgs)
    return precision_at_k, ndcg_at_k

# âœ… Run and print results
precision, ndcg = evaluate_model(model, test_data, k=10)
print(f"\nâœ… Precision@5: {precision:.4f}")
print(f"âœ… NDCG@5: {ndcg:.4f}")

